import re
import streamlit as st
from openai import OpenAI
import os
from qdrant_client import QdrantClient
from qdrant_client.http import models as qdrant_models
import uuid
import datetime

# ğŸ”¥ Welcome to the Underground ğŸ”¥
# This is a slick journaling app that stores and retrieves entries from Qdrant,
# and summons the power of OpenAI's GPT-4o-mini to reflect on your thoughts.
# Itâ€™s got embeddings, a bit of authentication, and a smooth streaming chat.

client = OpenAI()

# --- ğŸ´â€â˜ ï¸ QDRANT SETUP (VECTOR DATABASE) ---
# Qdrant: The unsung hero storing high-dimensional vectors.
QDRANT_URL = "https://67bd4e7c-9e18-4183-8655-cb368b598d90.europe-west3-0.gcp.cloud.qdrant.io"
QDRANT_API_KEY = st.secrets["QDRANT_API_KEY"]

qdrant_client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY,
    prefer_grpc=False  # Because sometimes, gRPC is just overkill.
)

COLLECTION_NAME = "journal_entries"


def init_qdrant_collection():
    """ Ensures the Qdrant collection exists, otherwise creates it. """
    vector_size = 1536  # Matches OpenAI's "text-embedding-3-small"
    try:
        collections_info = qdrant_client.get_collections()
        collection_names = [col.name for col in collections_info.collections]
    except Exception as e:
        st.error(f"Error fetching collections: {e}")
        collection_names = []

    if COLLECTION_NAME in collection_names:
        st.write(" ")  # Don't want any confirmation message atm, just works.
    else:
        qdrant_client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=qdrant_models.VectorParams(size=vector_size, distance="Cosine")
        )
        st.write("ğŸš€ Created a new collection in Qdrant.")


# --- ğŸ¤– TEXT EMBEDDING ---
def embed_text(text: str) -> list[float]:
    """ Converts input text into a vector embedding using OpenAI. """
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding  # Returning pure vector goodness.


# --- ğŸ” RETRIEVING RELEVANT ENTRIES ---
def retrieve_relevant_entries(user_id, query_text, top_k=3):
    """
    Fetches the top K most relevant journal entries based on vector similarity.
    Uses cosine distance because, well, thatâ€™s what the cool kids use.
    """
    query_embedding = embed_text(query_text)
    response = qdrant_client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_embedding,
        limit=top_k,
        with_payload=True,
        with_vectors=False
    )

    top_entries = []
    for point in response.points:
        payload = point.payload
        title = payload.get("title") or payload.get("text", "N/A")
        creator = payload.get("creator", "N/A")
        date = payload.get("post_date") or payload.get("timestamp", "N/A")
        content = payload.get("content", "N/A")

        entry_str = f"ğŸ“– **{title}**\nğŸ—“ï¸ {date}\n\n{content}"
        top_entries.append(entry_str)

    return top_entries


# --- ğŸ§  STREAMING GPT RESPONSE ---
def stream_gpt_response(question, relevant_texts, chat_container):
    """
    Streams GPT response dynamically.
    - Retrieves journal entries as context.
    - Generates a reflection (always in Swedish ğŸ‡¸ğŸ‡ª).
    - Uses smart placeholders to avoid flickering UI updates.
    """
    if relevant_texts:
        context_str = "\n\n".join(relevant_texts)
    else:
        context_str = "Jag hittar inget om det i din dagbok ğŸ˜."

    system_prompt = """
    Du Ã¤r en empatisk och insiktsfull dagbokskompanjon. 
    Din uppgift Ã¤r att hÃ¤mta relevanta dagboksinlÃ¤gg och ge en reflektion.
    Alla svar mÃ¥ste vara pÃ¥ svenska.

    - **InlÃ¤gg:** 
      [Visa inlÃ¤ggen]

    - **Reflektion:** 
      [Din insikt hÃ¤r]
    """

    user_prompt = f"""
    **Relevanta dagboksinlÃ¤gg:**  

    {context_str}  

    **AnvÃ¤ndarens frÃ¥ga:**  
    {question}
    """

    response_stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        stream=True
    )

    full_response = ""
    message_buffer = ""
    journal_placeholder = chat_container.empty()
    reflection_placeholder = chat_container.empty()

    journal_text = ""
    reflection_text = ""


    # --- ğŸ“¸ SET LOCAL AVATAR IMAGE ---
    # Define the local image path (inside "static/" folder)
    avatar_filename = "static/noras.PNG"
    avatar_path = os.path.join(os.path.dirname(__file__), avatar_filename)

    # Check if the file exists and serve it correctly
    if os.path.exists(avatar_path):
        # Streamlit needs a public URL or in-memory image, so we use `st.image()` workaround
        avatar_image = avatar_path
    else:
        st.warning(f"âš ï¸ Avatar image '{avatar_filename}' not found! Using fallback URL.")
        avatar_image = "https://raw.githubusercontent.com/your-user/your-repo/main/static/noras.png"  # Replace with actual hosted URL

    for chunk in response_stream:
        token = getattr(chunk.choices[0].delta, "content", "") or ""
        if not token:
            continue
        full_response += token
        message_buffer += token

        if "Reflektion:" in message_buffer:
            parts = message_buffer.split("Reflektion:", 1)
            journal_text = parts[0].strip()
            reflection_text = "Reflektion:" + parts[1].strip()
        else:
            journal_text = message_buffer
            reflection_text = ""

        with journal_placeholder:
            st.chat_message("system").markdown(f"**InlÃ¤gg:**\n\n{journal_text}")

        with reflection_placeholder:
            # âœ… FIX: Use `avatar_image`, whether it's a valid local file or fallback URL
            st.chat_message("assistant", avatar=avatar_image).markdown(reflection_text)

    return full_response

# --- ğŸš€ MAIN APP ---
def main():
    """ Streamlit UI and authentication flow """
    st.set_page_config(page_title="Log.AI", layout="wide")
    init_qdrant_collection()

    if "logged_in" not in st.session_state:
        st.session_state.logged_in = False
    if "user_id" not in st.session_state:
        st.session_state.user_id = None

    if not st.session_state.logged_in:
        with st.container():
            st.subheader("ğŸ” Login")
            username = st.text_input("Username")
            password = st.text_input("Password", type="password")

            if st.button("Login"):
                stored_username = st.secrets.get("ADMIN_USERNAME", "admin")
                stored_password = st.secrets.get("ADMIN_PASSWORD", "password")

                if username == stored_username and password == stored_password:
                    st.session_state.logged_in = True
                    st.session_state.user_id = username
                    st.success(f"ğŸ‰ Logged in as {username}")
                else:
                    st.error("ğŸš¨ Invalid credentials")
        return

    with st.container():
        st.subheader("ğŸ‘±â€â™€ï¸ Saga - Din Dagbokskompanjon")
        user_question = st.text_input("Vad tÃ¤nker du pÃ¥?...", key="user_input")

    chat_container = st.container()

    if st.button("ğŸ” SÃ¶k"):
        if user_question.strip():
            if st.session_state.user_id is None:
                st.error("âš ï¸ User ID saknas. Logga in fÃ¶rst.")
                return

            relevant_entries = retrieve_relevant_entries(st.session_state.user_id, user_question, top_k=5)

            with st.expander("ğŸ” FelsÃ¶kning", expanded=True):
                st.write("ğŸ“š **Top K hÃ¤mtade inlÃ¤gg:**")
                st.write(relevant_entries)

            stream_gpt_response(user_question, relevant_entries, chat_container)
        else:
            st.warning("âš ï¸ Skriv en frÃ¥ga fÃ¶rst.")


if __name__ == "__main__":
    main()
